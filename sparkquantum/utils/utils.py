import os
import sys
import math
import tempfile as tf

__all__ = ['Utils']


class Utils():
    """A class that provides utility static methods."""

    DumpingModeUniqueFile = 0
    """
    DumpingModeUniqueFile : int
        Indicate that the mathematical entity will be dumped to a unique file in disk. This is not suitable for large working sets,
        as all data must be collected to the driver node of the cluster and may not fit into memory.
    """
    DumpingModePartFiles = 1
    """
    DumpingModePartFiles : int
        Indicate that the mathematical entity will be dumped to part-* files in disk. This is done in a parallel/distributed way.
        This is the default behaviour.
    """
    MatrixCoordinateDefault = 0
    """
    MatrixCoordinateDefault : int
        Indicate that the `Matrix` object must have its entries stored as (i,j,value) coordinates.
    """
    MatrixCoordinateMultiplier = 1
    """
    MatrixCoordinateMultiplier : int
        Indicate that the `Matrix` object must have its entries stored as (j,(i,value)) coordinates. This is mandatory
        when the object is the multiplier operand.
    """
    MatrixCoordinateMultiplicand = 2
    """
    MatrixCoordinateMultiplicand : int
        Indicate that the `Matrix` object must have its entries stored as (i,(j,value)) coordinates. This is mandatory
        when the object is the multiplicand operand.
    """
    StateRepresentationFormatCoinPosition = 0
    """
    StateRepresentationFormatCoinPosition : int
        Indicate that the quantum system is represented as the kronecker product between the coin and position subspaces.
    """
    StateRepresentationFormatPositionCoin = 1
    """
    StateRepresentationFormatPositionCoin : int
        Indicate that the quantum system is represented as the kronecker product between the position and coin subspaces.
    """
    StateDumpingFormatIndex = 0
    """
    StateDumpingFormatIndex : int
        Indicate that the quantum system will be dumped to disk with the format (i,value).
    """
    StateDumpingFormatCoordinate = 1
    """
    StateDumpingFormatCoordinate : int
        Indicate that the quantum system will be dumped to disk with the format (i1,x1,...,in,xn,value),
        for one-dimensional meshes, or (i1,j1,x1,y1,...,in,jn,xn,yn,value), for two-dimensional meshes,
        for example, where n is the number of particles.
    """
    KroneckerModeBroadcast = 0
    """
    KroneckerModeBroadcast : int
        Indicate that the kronecker operation will be performed with the right operand in a broadcast variable.
        Suitable for small working sets, providing the best performance due to all the data be located
        locally in every node of the cluster.
    """
    KroneckerModeDump = 1
    """
    KroneckerModeDump : int
        Indicate that the kronecker operation will be performed with the right operand previously dumped to disk.
        Suitable for working sets that do not fit in cache provided by Spark.
    """
    BrokenLinksGenerationModeBroadcast = 0
    """
    BrokenLinksGenerationModeBroadcast : int
        Indicate that the broken links/mesh percolation will be generated by a broadcast variable
        containing the edges that are broken. Suitable for small meshes, providing the best performance
        due to all the data be located locally in every node of the cluster.
    """
    BrokenLinksGenerationModeRDD = 1
    """
    BrokenLinksGenerationModeRDD : int
        Indicate that the broken links/mesh percolation will be generated by a RDD containing the edges that are broken.
        Suitable for meshes that their corresponding data do not fit in the cache provided by Spark. Also, with this mode,
        a left join is performed, making it not the best mode.
    """

    ConfigDefaults = {
        'quantum.dumpingGlue': ' ',
        'quantum.dumpingCompressionCodec': None,
        'quantum.math.roundPrecision': 10,
        'quantum.math.dumpingMode': DumpingModePartFiles,
        'quantum.cluster.numPartitionsSafetyFactor': 1.3,
        'quantum.cluster.useSparkDefaultNumPartitions': 'False',
        'quantum.cluster.totalCores': 1,
        'quantum.cluster.maxPartitionSize': 64 * 10 ** 6,
        'quantum.dtqw.mesh.brokenLinks.generationMode': BrokenLinksGenerationModeBroadcast,
        'quantum.dtqw.interactionOperator.checkpoint': 'False',
        'quantum.dtqw.walkOperator.checkpoint': 'False',
        'quantum.dtqw.walkOperator.kroneckerMode': KroneckerModeBroadcast,
        'quantum.dtqw.walkOperator.tempPath': './',
        'quantum.dtqw.walk.checkpointStates': 'False',
        'quantum.dtqw.walk.checkpointingFrequency': -1,
        'quantum.dtqw.walk.dumpStates': 'False',
        'quantum.dtqw.walk.dumpingFrequency': -1,
        'quantum.dtqw.walk.dumpingPath': './',
        'quantum.dtqw.walk.checkUnitary': 'False',
        'quantum.dtqw.walk.dumpStatesPDF': 'False',
        'quantum.dtqw.state.representationFormat': StateRepresentationFormatCoinPosition,
        'quantum.dtqw.state.dumpingFormat': StateDumpingFormatIndex,
        'quantum.dtqw.profiler.logExecutors': 'False'
    }
    """
    ConfigDefaults: dict
        Dict with the default values for all accepted configurations of the package.
    """

    def __init__(self):
        pass

    @staticmethod
    def is_shape(shape):
        """Check if an object is a shape, i.e., a list or a tuple.

        Parameters
        ----------
        shape :
            The object to be checked if it is a shape.

        Returns
        -------
        bool
            True if argument is a shape, False otherwise.

        """
        return isinstance(shape, (list, tuple))

    @staticmethod
    def broadcast(sc, data):
        """Broadcast some data.

        Parameters
        ----------
        sc : `SparkContext`
            The `SparkContext` object.
        data :
            The data to be broadcast.

        Returns
        -------
        `Broadcast`
            The `Broadcast` object that contains the broadcast data.

        """
        return sc.broadcast(data)

    @staticmethod
    def get_conf(sc, config_str):
        """Get a configuration value from the `SparkContext` object.

        Parameters
        ----------
        sc : `SparkContext`
            The `SparkContext` object.
        config_str : str
            The configuration string to have its correspondent value obtained.

        Returns
        -------
        str
            The configuration value or `None` if the configuration is not found.

        """
        c = sc.getConf().get(config_str)

        if not c:
            if config_str not in Utils.ConfigDefaults:
                return None
            return Utils.ConfigDefaults[config_str]

        return c

    @staticmethod
    def change_coordinate(rdd, old_coord, new_coord=MatrixCoordinateDefault):
        """Change the coordinate format of a `Matrix` object's RDD.

        Parameters
        ----------
        rdd : `RDD`
            The `Matrix` object's RDD to have its coordinate format changed.
        old_coord : int
            The original coordinate format of the `Matrix` object's RDD.
        new_coord : int
            The new coordinate format. Default value is `Utils.MatrixCoordinateDefault`.

        Returns
        -------
        `RDD`
            A new `RDD` with the coordinate format changed.

        """
        if old_coord == Utils.MatrixCoordinateMultiplier:
            if new_coord == Utils.MatrixCoordinateMultiplier:
                return rdd
            elif new_coord == Utils.MatrixCoordinateMultiplicand:
                return rdd.map(
                    lambda m: (m[1][0], (m[0], m[1][1]))
                )
            else:  # Utils.MatrixCoordinateDefault
                return rdd.map(
                    lambda m: (m[1][0], m[0], m[1][1])
                )
        elif old_coord == Utils.MatrixCoordinateMultiplicand:
            if new_coord == Utils.MatrixCoordinateMultiplier:
                return rdd.map(
                    lambda m: (m[1][0], (m[0], m[1][1]))
                )
            elif new_coord == Utils.MatrixCoordinateMultiplicand:
                return rdd
            else:  # Utils.MatrixCoordinateDefault
                return rdd.map(
                    lambda m: (m[0], m[1][0], m[1][1])
                )
        else:  # Utils.MatrixCoordinateDefault
            if new_coord == Utils.MatrixCoordinateMultiplier:
                return rdd.map(
                    lambda m: (m[1], (m[0], m[2]))
                )
            elif new_coord == Utils.MatrixCoordinateMultiplicand:
                return rdd.map(
                    lambda m: (m[0], (m[1], m[2]))
                )
            else:  # Utils.MatrixCoordinateDefault
                return rdd

    @staticmethod
    def filename(mesh_filename, steps, num_particles):
        """Build a filename concatenating the parameters.

        Parameters
        ----------
        mesh_filename : str
            The generated name for the used mesh.
        steps : int
            The number of steps of the walk.
        num_particles : int
            The number of particles in the walk.

        Returns
        -------
        str
            The filename built.

        """
        return "{}_{}_{}".format(mesh_filename, steps, num_particles)

    @staticmethod
    def get_precendent_type(type1, type2):
        """Compare and return the most precendent type between two types.

        Parameters
        ----------
        type1 : type
            The first type to be compared with.
        type2 : type
            The second type to be compared with.

        Returns
        -------
        type
            The type with most precendent order.

        """
        if type1 == complex or type2 == complex:
            return complex

        if type1 == float or type2 == float:
            return float

        return int

    @staticmethod
    def get_size_of_type(data_type):
        """Get the size in bytes of a Python type.

        Parameters
        ----------
        data_type : type
            The Python type to have its size calculated.

        Returns
        -------
        int
            The size of the Python type in bytes.

        """
        return sys.getsizeof(data_type())

    @staticmethod
    def get_num_partitions(spark_context, expected_size):
        """Calculate the number of partitions for a `RDD` based on its expected size in bytes.

        Parameters
        ----------
        spark_context : `SparkContext`
            The `SparkContext` object.
        expected_size : int
            The expected size in bytes of the RDD.

        Returns
        -------
        int
            The number of partitions for the RDD.

        Raises
        ------
        `ValueError`

        """
        safety_factor = float(Utils.get_conf(spark_context, 'quantum.cluster.numPartitionsSafetyFactor'))
        num_partitions = None

        if Utils.get_conf(spark_context, 'quantum.cluster.useSparkDefaultNumPartitions') == 'False':
            num_cores = Utils.get_conf(spark_context, 'quantum.cluster.totalCores')

            if not num_cores:
                raise ValueError("invalid number of total cores in the cluster: {}".format(num_cores))

            num_cores = int(num_cores)
            max_partition_size = int(Utils.get_conf(spark_context, 'quantum.cluster.maxPartitionSize'))
            num_partitions = math.ceil(safety_factor * expected_size / max_partition_size / num_cores) * num_cores

        return num_partitions

    @staticmethod
    def append_slash_dir(path):
        """Append a slash in a path if it does not end with one.

        Parameters
        ----------
        path : str
            The directory name with its path.

        """
        if not path.endswith('/'):
            path += '/'

        return path

    @staticmethod
    def create_dir(path):
        """Create a directory in the filesystem.

        Parameters
        ----------
        path : str
            The directory name with its path.

        Raises
        ------
        `ValueError`

        """
        if os.path.exists(path):
            if not os.path.isdir(path):
                raise ValueError("'{}' is an invalid path".format(path))
        else:
            os.makedirs(path)

    @staticmethod
    def get_temp_path(d):
        """Create a temp directory in the filesystem.

        Parameters
        ----------
        d : str
            The name of the temp path.

        """
        tmp_file = tf.NamedTemporaryFile(dir=d)
        tmp_file.close()

        return tmp_file.name

    @staticmethod
    def remove_path(path):
        """Delete a directory in the filesystem.

        Parameters
        ----------
        path : str
            The directory name with its path.

        Raises
        ------
        `ValueError`

        """
        if os.path.exists(path):
            if os.path.isdir(path):
                if path != '/':
                    for i in os.listdir(path):
                        Utils.remove_path(path + '/' + i)
                    os.rmdir(path)
            else:
                os.remove(path)

    @staticmethod
    def clear_path(path):
        """Empty a directory in the filesystem.

        Parameters
        ----------
        path : str
            The directory name with its path.

        Raises
        ------
        `ValueError`

        """
        if os.path.exists(path):
            if os.path.isdir(path):
                if path != '/':
                    for i in os.listdir(path):
                        Utils.remove_path(path + '/' + i)
            else:
                raise ValueError("'{}' is an invalid path".format(path))

    @staticmethod
    def get_size_of_path(path):
        """Get the size in bytes of a directory in the filesystem.

        Parameters
        ----------
        path : str
            The directory name with its path.

        Returns
        -------
        int
            The size in bytes of the directory.

        """
        if os.path.isdir(path):
            size = 0
            for i in os.listdir(path):
                size += Utils.get_size_of_path(path + '/' + i)
            return size
        else:
            return os.stat(path).st_size
